{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Turorial description","text":"<p>This tutorial is given in the context of the Scynergy 2026 event and EPICURE, participants will explore how to program High-Performance GPU/FPGA cards using Rust. </p>"},{"location":"#agenda","title":"Agenda:","text":"<pre><code>---\nconfig:\n  theme: 'dark'\n---\nflowchart TD\n    A[Rust] --&gt; B{Accelerators}\n    B --&gt; C[GPU]\n    B --&gt; D[FPGA]\n    C --&gt; E[Host code in Rust \n            Device code in CUDA C/C++]\n    C --&gt; F[Host code in Rust \n            Device code in Rust-CUDA]\n    D --&gt; G[Host code in Rust \n            Device code in OpenCL]</code></pre>"},{"location":"#why-rust","title":"Why Rust ?","text":"<ul> <li> <p>Memory safety without garbage collection: Rust\u2019s ownership and borrowing system catches bugs like null pointers, use-after-free, and data races at compile time. You get C/C++-level control without the usual foot-guns.</p> </li> <li> <p>High performance: Rust compiles to native code and has zero-cost abstractions, meaning you can write clean, high-level code that runs as fast as low-level code. Great for systems programming, game engines, and performance-critical services.</p> </li> <li> <p>Fearless concurrency: Rust makes it hard to write unsafe concurrent code. The compiler enforces thread safety, so many race conditions simply won\u2019t compile. This is a big deal for modern, multi-core software.</p> </li> <li> <p>Excellent tooling and ecosystem: Cargo (Rust\u2019s build system and package manager) is fast, simple, and a joy to use. Add in great compiler error messages, formatting tools, and a growing ecosystem, and developer experience is genuinely top-tier.</p> </li> </ul>"},{"location":"#why-rust-on-accelerators","title":"Why Rust on accelerators ?","text":"<ul> <li> <p>GPU and FPGA bugs are hard to debug and often fail silently or nondeterministically.</p> </li> <li> <p>Rust helps by:</p> <ul> <li> <p>Preventing data races at compile time</p> </li> <li> <p>Enforcing aliasing rules (huge for shared buffers, DMA, HBM)</p> </li> <li> <p>Making unsafe code explicit, so low-level accelerator code is clearly isolated and audited</p> </li> </ul> </li> </ul> <p>This is especially valuable when writing kernels, drivers, or host\u2013device interfaces.</p>"},{"location":"#what-will-you-learn","title":"What will you learn ?","text":"<p>In this course, you will learn to:</p> <ul> <li> <p>How to build and use OpenCL kernels with Rust on FPGAs and GPUs</p> </li> <li> <p>How to build and use CUDA kernels with Rust on NVIDIA GPUs</p> </li> <li> <p>How to build and use Rust-CUDA kernels with Rust on NVIDIA GPUs</p> </li> </ul>"},{"location":"#what-are-we-going-to-do","title":"What are we going to do ?","text":"<p>In order to practice how to use Rust on MeluXina's accelerators, we will code a small convolution kernel for edge detection:</p> <p></p> <p></p> <p>Remark</p> <p>This course is not intended to be exhaustive. It is NOT a Rust course neither an OpenCL/CUDA course.</p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<ul> <li> <p>This course is for students, researchers, engineers wishing to discover how to use Rust to program FPGA and GPUs on a High High-Performance Computing platform. </p> </li> <li> <p>Participants should still have some basic notions of Rust programming. </p> </li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>The Rust book</li> <li>The Rust by example</li> <li>The opencl3 crate documentation</li> <li>The cust crate documentation</li> </ul>"},{"location":"#about-this-course","title":"About this course","text":"<p>This course has been developed by the Supercomputing Application Services group at LuxProvide in the context of the EPICURE project.</p>"},{"location":"rust/","title":"Rust on MeluXina","text":""},{"location":"rust/#introduction","title":"Introduction","text":""},{"location":"rust/#meluxina-nvidia-a100","title":"MeluXina NVIDIA A100","text":"<p>The 200 Accelerator Module GPU nodes are based on the NVIDIA A100 GPU-AI accelerators. Each GPU has 40GB of on-board High Bandwidth Memory and the 4 GPUs on each compute node are linked together with the NVLink 3 interconnect (1,555GB/s memory bandwidth). The A100 supports the Multi-Instance GPU feature, allowing the GPUs to be split up into 7 'independent' with 5 GB each.</p> <p>The A100 with 108 SMs provides higher performance and new features compared to the previous Volta and Turing generations. Key SM features are briefly highlighted below, check out the Ampere architecture white paper for additional details. NVIDIA also provides a dedicated tuning guide for the Ampere-based GPUs, enabling developers to take advantage of the new features.</p>"},{"location":"rust/#meluxina-bittware-520n-mx-fpgas","title":"MeluXina Bittware 520N-MX FPGAs","text":"<p>Each of the 20 MeluXina FPGA compute nodes comprise two BittWare 520N-MX FPGAs based on the Intel Stratix 10 FPGA chip. Designed for compute acceleration, the 520N-MX are PCIe boards featuring Intel\u2019s Stratix 10 MX2100 FPGA with integrated HBM2 memory. The size and speed of HBM2 (16GB at up to 512GB/s) enables acceleration of memory-bound applications. Programming with high abstraction C, C++, and OpenCLis possible through an specialized board support package (BSP) for the Intel OpenCL SDK. For more details see the dedicated BittWare product page.</p> <p></p> <p>Intel Stratix 520N-MX Block Diagram.</p>"},{"location":"rust/#setup","title":"Setup","text":"<ul> <li> <p>Clone first the repository and enter the <code>RustOnAccelerators</code> folder <pre><code>git clone https://github.com/LuxProvide/RustOnAccelerators.git\ncd RustOnAccelerators/code\n</code></pre></p> </li> <li> <p>You need first to obtain an interactive job on the fpga partition and load a module <pre><code>salloc -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; -t 01:00:00 -q default -p gpu -N1\n# OR \n# salloc -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; -t 01:00:00 -q default -p fpga -N1\n\n$tree -L 1\n.\n\u251c\u2500\u2500 rust-cuda\n\u251c\u2500\u2500 rust-nvcc\n\u251c\u2500\u2500 rust-opencl-fpga\n\u251c\u2500\u2500 rust-opencl-gpu\n\u251c\u2500\u2500 rust-toolchain.toml\n\u251c\u2500\u2500 setup_rustfpga.sh\n\u251c\u2500\u2500 setup_rustgpu.sh\n\u2514\u2500\u2500 utils\n</code></pre></p> </li> <li> <p>The code folder contains 5 crates:</p> </li> <li> <p><code>rust-cuda</code>: crate using Rust for the host and Rust for the device code</p> </li> <li><code>rust-nvcc</code>: crate using Rust for the host and C/C++ CUDA for the device code</li> <li><code>rust-opencl-fpga</code>: crate using Rust for the host and C/C++ OpenCL for the device code with FPGAs as target devices</li> <li><code>rust-opencl-gpu</code>: crate using Rust for the host and C/C++ OpenCL for the device code with GPUs as target devices</li> <li> <p><code>utils</code>: crate containing necessary libraries to load and save images</p> </li> <li> <p>Two scripts are also here at your disposal:</p> </li> <li><code>setup_rustfpga.sh</code>: a script to be sourced to setup Rust and the toolchain as well as the required modules to use FPGAs</li> <li> <p><code>setup_rustgpu.sh</code>: a script to be sourced to setup Rust and the toolchain as well as the required modules to GPUs</p> </li> <li> <p>A toml configuration file to configure the toolchain</p> </li> </ul>"},{"location":"rust/#convolution-explained","title":"Convolution explained","text":"<ul> <li>Below is a serial CPU code to apply image convolution.  </li> <li>The two external loops apply the kernel for each given pixel of the matrix</li> <li>The two internal loops through the kernel and multiply the weights to the current pixel's neighbors</li> <li>Finally, the current pixel value will be the reduced sum of all weighted pixel's neighbors</li> </ul> convolution C/C++ function<pre><code>void convolution(float *img, float *kernel, float *imgf, int Nx, int Ny, int kernel_size)\n{\n\n  float sum = 0;\n  int center = (kernel_size -1)/2;\n  int ii, jj;\n\n  for (int i = center; i&lt;(Ny-center); i++)\n    for (int j = center; j&lt;(Nx-center); j++){\n      sum = 0;\n      for (int ki = 0; ki&lt;kernel_size; ki++)\n            for (int kj = 0; kj&lt;kernel_size; kj++){\n                  ii = kj + j - center;\n                  jj = ki + i - center;\n                  sum+=img[jj*Nx+ii]*kernel[ki*kernel_size + kj];\n            }\n          imgf[i*Nx +j] = sum;\n      }\n }\n</code></pre>"},{"location":"rust/#cargo-rusts-build-system-and-package-manager","title":"Cargo: Rust\u2019s Build System and Package Manager","text":"<ul> <li>Cargo is the official build system and package manager for Rust. If you use Rust, you use Cargo\u2014it\u2019s the backbone of the ecosystem.</li> <li>Think of Cargo as doing for Rust what <code>npm</code> does for JavaScript or <code>pip + setuptools</code> do for Python, but tightly integrated and opinionated (in a good way).</li> </ul>"},{"location":"rust/#what-cargo-does","title":"What Cargo Does","text":""},{"location":"rust/#1-project-creation","title":"1. Project Creation","text":"<p>Cargo generates a standard project structure.</p> <pre><code>cargo new my_app\n</code></pre> <p>Resulting layout: <pre><code>my_app/\n\u251c\u2500\u2500 Cargo.toml   # Project configuration &amp; dependencies\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 main.rs  # Application entry point\n</code></pre></p>"},{"location":"rust/#2-dependency-management","title":"2. Dependency Management","text":"<p>Dependencies are declared in <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\nserde = \"1.0\"\nrand = \"0.8\"\n</code></pre> <p>Cargo automatically: - Resolves compatible versions - Downloads crates from crates.io - Locks exact versions in <code>Cargo.lock</code> - Builds dependencies in the correct order</p>"},{"location":"rust/#3-building-code","title":"3. Building Code","text":"<pre><code>cargo build\ncargo build --release\n</code></pre> <p>Build artifacts are stored in: <pre><code>target/debug/\ntarget/release/\n</code></pre></p>"},{"location":"rust/#4-running-programs","title":"4. Running Programs","text":"<pre><code>cargo run\n</code></pre>"},{"location":"rust/#building-third-party-non-rust-code","title":"Building third-party non-Rust code","text":"<ul> <li>In this tutorial, we will need to rely on third-party code to build device code</li> <li>So, we do an extensive use of the build-scripts feature</li> <li>You can add an additional <code>build.rs</code> script inside your crate to trigger the compilation of a third-party code:</li> </ul> <pre><code>// Example custom build script.\nfn main() {\n    // Tell Cargo that if the given file changes, to rerun this build script.\n    println!(\"cargo::rerun-if-changed=src/hello.c\");\n    // Use the `cc` crate to build a C file and statically link it.\n    cc::Build::new()\n        .file(\"src/hello.c\")\n        .compile(\"hello\");\n}\n</code></pre> <ul> <li>The above example provided by the official Cargo documentation shows how to build a C code </li> <li>Just before building your package, Cargo will compile the <code>build.rs</code> file and run it</li> <li>One really important feature is the way the script interact with Cargo. Using the <code>println!</code> macro, you can instruct Cargo no perform or not some tasks</li> <li>In the above example, Cargo will only rerun the script if the <code>src/hello.c</code> file has changed</li> <li>In order to build the device code for GPUs and FPGAs, we will use this mechanism. </li> </ul>"},{"location":"rust_cuda/","title":"A Rust host code executing CUDA kernel","text":""},{"location":"rust_cuda/#host-code","title":"Host code","text":"./code/rust-nvcc/src/main.rs<pre><code>use cust::prelude::*;\nuse getargs::{Arg, Options};\nuse std::error::Error;\nuse utils::{load_gray_f32, save_gray_f32};\n\n// Generated PTX file will be located in OUT_DIR\nstatic PTX: &amp;str = include_str!(concat!(env!(\"OUT_DIR\"), \"/conv2d_gray_f32.ptx\"));\n\nfn run(buffer: &amp;mut [f32], width: u32, height: u32) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    // Define kernel\n    let ksize = 3;\n    let weights: Vec&lt;f32&gt; = vec![0.0, 1.0, 0.0, 1.0, -4.0, 1.0, 0.0, 1.0, 0.0];\n\n    // initialize CUDA, this will pick the first available device and will\n    // make a CUDA context from it.\n    // We don't need the context for anything but it must be kept alive.\n    let _ctx = cust::quick_init()?;\n\n    // Make the CUDA module, modules just house the GPU code for the kernels we created.\n    // they can be made from PTX code, cubins, or fatbins.\n    let module = Module::from_ptx(PTX, &amp;[])?;\n\n    // make a CUDA stream to issue calls to. You can think of this as an OS thread but for dispatching\n    // GPU calls.\n    let stream = Stream::new(StreamFlags::NON_BLOCKING, None)?;\n\n    // Create timing events\n    let start = Event::new(EventFlags::DEFAULT)?;\n    let stop = Event::new(EventFlags::DEFAULT)?;\n\n    // allocate the GPU memory needed to house our numbers and copy them over.\n    let input_buf = DeviceBuffer::from_slice(buffer)?;\n    let weights_buf = weights.as_slice().as_dbuf()?;\n\n    let output_buf = unsafe { DeviceBuffer::&lt;f32&gt;::uninitialized(buffer.len())? };\n\n    // retrieve the `vecadd` kernel from the module so we can calculate the right launch config.\n    let conv2d_gray_f32 = module.get_function(\"conv2d_gray_f32\")?;\n\n    let block_size = (16u32, 16u32); // 256 threads\n\n    let grid_size = (\n        (width + block_size.0 - 1) / block_size.0,\n        (height + block_size.1 - 1) / block_size.1,\n    );\n\n    println!(\n        \"using {:?} blocks and {:?} threads per block\",\n        grid_size, block_size\n    );\n\n    start.record(&amp;stream)?;\n\n    // Actually launch the GPU kernel. This will queue up the launch on the stream, it will\n    // not block the thread until the kernel is finished.\n    unsafe {\n        launch!(\n            // slices are passed as two parameters, the pointer and the length.\n            conv2d_gray_f32&lt;&lt;&lt;grid_size, block_size, 0, stream&gt;&gt;&gt;(\n                input_buf.as_device_ptr(),\n                output_buf.as_device_ptr(),\n                weights_buf.as_device_ptr(),\n                width,\n                height,\n                ksize)\n        )?;\n    }\n\n    stop.record(&amp;stream)?;\n\n    // copy back the data from the GPU.\n    //output_buf.copy_to(&amp;mut out)?;\n    output_buf.copy_to(buffer)?;\n\n    println!(\n        \"kernel execution duration (ms): {}\",\n        Event::elapsed_time_f32(&amp;stop, &amp;start)?\n    );\n\n    Ok(())\n}\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let mut args = std::env::args().skip(1).collect::&lt;Vec&lt;_&gt;&gt;();\n\n    if args.is_empty() {\n        args.push(String::from(\"--help\")); // help the user out :)\n    }\n    let mut opts = Options::new(args.iter().map(String::as_str));\n    let mut input_path = None;\n    let mut output_path = None;\n\n    while let Some(arg) = opts.next_arg().expect(\"argument parsing error\") {\n        match arg {\n            Arg::Short('h') | Arg::Long(\"help\") =&gt; {\n                eprintln!(\n                    r\"Usage: rust-nvcc [OPTIONS/ARGS] input ...\n                     This command execute an CUDA Convolution kernel on GPU.\n                     -h, --help   display this help and exit\n                     -o, --output path to record output image\"\n                );\n            }\n            Arg::Short('o') | Arg::Long(\"output\") =&gt; {\n                output_path = opts.value_opt();\n            }\n            Arg::Positional(arg) =&gt; {\n                let metadata = std::fs::metadata(arg);\n                match metadata {\n                    Ok(m) =&gt; {\n                        if !m.is_file() {\n                            panic!(\"{arg:?} is not a file\");\n                        } else {\n                            input_path = Some(arg);\n                        }\n                    }\n                    Err(e) =&gt; {\n                        panic!(\"Error: {e:?}\");\n                    }\n                }\n            }\n            _ =&gt; {}\n        }\n    }\n\n    let (mut buffer, w, h) =\n        load_gray_f32(input_path.unwrap()).expect(\"Cannot read image located at {arg}\");\n    let status = run(&amp;mut buffer, w, h);\n    match status {\n        Ok(_) =&gt; {\n            if let Some(p) = output_path {\n                save_gray_f32(p, &amp;buffer, w, h).expect(\"Cannot save image at {p}\");\n            } else {\n                save_gray_f32(input_path.unwrap(), &amp;buffer, w, h)\n                    .expect(\"Cannot save image at {arg}\");\n            }\n            println!(\"Execution complete\");\n        }\n        Err(e) =&gt; {\n            panic!(\"ClError: {e:?}\");\n        }\n    }\n    Ok(())\n}\n</code></pre>"},{"location":"rust_cuda/#device-code","title":"Device code","text":""},{"location":"rust_cuda/#cc-device-code-building-the-cuda-kernel-with-nvcc","title":"C/C++ device code: building the CUDA kernel with <code>nvcc</code>","text":"./code/rust-nvcc/kernels/conv2d_gray_f32.cu<pre><code>#define MAX_K 31\n\nextern \"C\" __global__ void conv2d_gray_f32(const float *input, float *output,\n                                           const float *weights,\n                                           const int width, const int height,\n                                           const int kSize) {\n\n  const int x = (int)blockIdx.x * blockDim.x + threadIdx.x;\n  const int y = (int)blockIdx.y * blockDim.y + threadIdx.y;\n  const int lx = (int)threadIdx.x;\n  const int ly = (int)threadIdx.y;\n  const int bx = (int)blockDim.x;\n\n  if (x &gt;= width || y &gt;= height)\n    return;\n\n  __shared__ float kLocal[MAX_K * MAX_K];\n\n  const int lid = ly * bx + lx;\n  if (lid &lt; kSize * kSize) {\n    kLocal[lid] = weights[lid];\n  }\n\n  __syncthreads();\n\n  const int r = (kSize - 1) / 2;\n  float acc = 0.0f;\n\n  // Convolution sum\n  for (int ky = 0; ky &lt; kSize; ++ky) {\n    int iy = y + ky - r;\n    iy = max(0, min(iy, height - 1));\n\n    const int rowBase = iy * width;\n\n    for (int kx = 0; kx &lt; kSize; ++kx) {\n      int ix = x + kx - r;\n      ix = max(0, min(ix, width - 1));\n\n      const float p = input[rowBase + ix];\n      const float w = kLocal[ky * kSize + kx];\n      acc += p * w;\n    }\n  }\n  output[y * width + x] = acc;\n}\n</code></pre> ./code/rust-nvcc/build.rs<pre><code>use std::env;\nuse std::path::PathBuf;\nuse std::process::Command;\n\nfn main() {\n    let manifest_dir = PathBuf::from(env::var(\"CARGO_MANIFEST_DIR\").unwrap());\n    let kernel_src = manifest_dir.join(\"kernels\").join(\"conv2d_gray_f32.cu\");\n\n    // O\u00f9 Cargo met les artefacts de build pour cette cible/profil\n    let out_dir = PathBuf::from(env::var(\"OUT_DIR\").unwrap());\n    let ptx_out = out_dir.join(\"conv2d_gray_f32.ptx\");\n\n    // Rebuild si le kernel change\n    println!(\"cargo:rerun-if-changed={}\", kernel_src.display());\n\n    // Compile en PTX\n    // -ptx : sortie PTX\n    // -O3 : optimisation\n    let status = Command::new(\"nvcc\")\n        .arg(\"-ptx\")\n        .arg(\"-O3\")\n        .arg(kernel_src.as_os_str())\n        .arg(\"-o\")\n        .arg(ptx_out.as_os_str())\n        .status()\n        .expect(\"Failed to run nvcc. Is CUDA toolkit installed and nvcc in PATH?\");\n\n    if !status.success() {\n        panic!(\"nvcc failed compiling {}\", kernel_src.display());\n    }\n\n    // Optionnel: expose le chemin du PTX au code Rust (via env compile-time)\n    println!(\"cargo:rustc-env=VEC_ADD_PTX={}\", ptx_out.display());\n}\n</code></pre>"},{"location":"rust_cuda/#rust-device-code-building-the-cuda-kernel-with-rust-cuda","title":"Rust device code: building the CUDA kernel with <code>Rust-CUDA</code>","text":"./code/rust-cuda/kernels/src/lib.rs<pre><code>use core::cmp::{max, min};\nuse core::mem::MaybeUninit;\nuse cuda_std::address_space;\nuse cuda_std::prelude::*;\n\npub const MAX_K: usize = 31;\n\n#[kernel]\n#[allow(improper_ctypes_definitions, clippy::missing_safety_doc)]\npub unsafe fn conv2d_gray_f32(\n    input: &amp;[f32],\n    output: *mut f32,\n    weights: &amp;[f32],\n    width: usize,\n    height: usize,\n    k_size: usize,\n) {\n    let x = (thread::block_idx_x() * thread::block_dim_x() + thread::thread_idx_x()) as usize;\n    let y = (thread::block_idx_y() * thread::block_dim_y() + thread::thread_idx_y()) as usize;\n    let lx = thread::thread_idx_x() as usize;\n    let ly = thread::thread_idx_y() as usize;\n    let bx = thread::block_dim_x() as usize;\n\n    if x &lt; width &amp;&amp; y &lt; height {\n        const MAX_K_2D: usize = MAX_K * MAX_K;\n\n        #[address_space(shared)]\n        static mut KLOCAL: [MaybeUninit&lt;f32&gt;; MAX_K_2D] = [MaybeUninit::uninit(); MAX_K_2D];\n\n        let lid = ly * bx + lx;\n        if lid &lt; k_size * k_size {\n            unsafe {\n                KLOCAL[lid].write(weights[lid]);\n            }\n        }\n\n        thread::sync_threads();\n\n        let r = (k_size - 1) / 2;\n        let mut acc: f32 = 0.0;\n\n        // Convolution sum\n        for ky in 0..k_size {\n            let mut iy = y + ky - r;\n            iy = max(0usize, min(iy, height - 1));\n            for kx in 0..k_size {\n                let mut ix = x + kx - r;\n                ix = max(0usize, min(ix, width - 1));\n                let p = input[iy * width + ix];\n                let w = unsafe { KLOCAL[ky * k_size + kx].assume_init() };\n                acc += p * w;\n            }\n        }\n        let o = unsafe { output.add(y * width + x) };\n        unsafe { *o = acc };\n    }\n}\n</code></pre> ./code/rust-cuda/build.rs<pre><code>use std::env;\nuse std::path;\n\nuse cuda_builder::CudaBuilder;\n\nfn main() {\n    println!(\"cargo::rerun-if-changed=build.rs\");\n    println!(\"cargo::rerun-if-changed=kernels\");\n\n    let out_path = path::PathBuf::from(env::var(\"OUT_DIR\").unwrap());\n    let manifest_dir = path::PathBuf::from(env::var(\"CARGO_MANIFEST_DIR\").unwrap());\n\n    CudaBuilder::new(manifest_dir.join(\"kernels\"))\n        .copy_to(out_path.join(\"conv2d_gray_f32.ptx\"))\n        .build()\n        .unwrap();\n}\n</code></pre>"},{"location":"rust_cuda/#execution-on-meluxina","title":"Execution on MeluXina","text":""},{"location":"rust_cuda/#interactive-execution","title":"Interactive execution","text":"<pre><code>salloc -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; -t 30:00 -q default -p gpu\ncd RustOnAccelerators/code\nsource setup_rustgpu.sh\n</code></pre> Rust-nvccRust-cuda <pre><code>cd ${CODE_ROOT}/rust-nvcc \ncargo build --release\n# Execute the code\n./target/release/rust-nvcc -orust-nvcc-image.png ../../data/original_image.png\n</code></pre> <pre><code>cd ${CODE_ROOT}/rust-cuda\ncargo build --release\n# Execute the code\n./target/release/rust-cuda -orust-cuda-image.png ../../data/original_image.png\n</code></pre>"},{"location":"rust_cuda/#batch-execution","title":"Batch execution","text":"<pre><code>cd RustOnAccelerators/code\nsbatch -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; launcher-rust-nvcc-cuda.sh\n</code></pre>"},{"location":"rust_cuda/#results","title":"Results","text":"<ul> <li>You should see the following results for both executions:</li> </ul> Original Convolution"},{"location":"rust_cuda/#explore-further","title":"Explore Further","text":"<ul> <li>Try to modify the kernel coefficients</li> <li>Try to change the original image</li> <li>Adapt the code for Tiled Matrix Multiplication</li> </ul>"},{"location":"rust_opencl/","title":"A Rust host code executing OpenCL kernel for FPGA","text":"<ul> <li>We are going to build the code in emulation mode</li> <li>We are going to run with a prepare FPGA image on the real FPGA cards</li> </ul>"},{"location":"rust_opencl/#host-code","title":"Host code","text":"./code/rust-opencl-fpga/src/main.rs<pre><code>use getargs::{Arg, Options};\nuse opencl3::command_queue::{CL_QUEUE_PROFILING_ENABLE, CommandQueue};\nuse opencl3::context::Context;\nuse opencl3::device::{CL_DEVICE_NOT_FOUND, CL_DEVICE_TYPE_ACCELERATOR, Device};\nuse opencl3::error_codes::{CL_INVALID_PLATFORM, ClError};\nuse opencl3::kernel::Kernel;\nuse opencl3::memory::{Buffer, CL_MEM_READ_ONLY, CL_MEM_WRITE_ONLY};\nuse opencl3::program::Program;\nuse opencl3::types::{CL_BLOCKING, CL_NON_BLOCKING, cl_event, cl_float, cl_uint};\nuse opencl3::{Result, platform};\nuse std::ptr;\nuse utils::{load_gray_f32, save_gray_f32};\n\nconst KERNEL_NAME: &amp;str = \"conv2d_gray_f32\";\nstatic FPGA_GEN_IMAGE: &amp;str = concat!(env!(\"OUT_DIR\"), \"/conv2d_gray_f32.aocx\");\n\nfn run(buffer: &amp;mut [f32], width: u32, height: u32) -&gt; Result&lt;()&gt; {\n    let buffer_size = (width * height) as usize;\n\n    // Define kernel\n    let ksize: cl_uint = 3;\n    let weights: Vec&lt;f32&gt; = vec![0.0, 1.0, 0.0, 1.0, -4.0, 1.0, 0.0, 1.0, 0.0];\n\n    let platforms: Vec&lt;platform::Platform&gt; = platform::get_platforms()?;\n    let intel_fpga_platform = platforms\n        .into_iter()\n        .find(|p| {\n            let name = p.name().unwrap_or_default();\n            name.to_lowercase().contains(\"fpga\")\n        })\n        .ok_or_else(|| ClError::from(CL_INVALID_PLATFORM))?;\n\n    let device_ids = intel_fpga_platform.get_devices(CL_DEVICE_TYPE_ACCELERATOR)?;\n\n    // Find a usable device for this application\n    let device_id = match device_ids.first() {\n        Some(ptr) =&gt; *ptr,\n        None =&gt; return Err(ClError::from(CL_DEVICE_NOT_FOUND)),\n    };\n\n    let device = Device::new(device_id);\n\n    println!(\"Executing on {}\", device.name()?);\n\n    // Create a Context on an OpenCL device\n    let context = Context::from_device(&amp;device)?;\n\n    // Create a command_queue on the Context's device\n    let queue = CommandQueue::create_default(&amp;context, CL_QUEUE_PROFILING_ENABLE)?;\n\n    // Read bistream\n    let aocx_path = std::env::var(\"FPGA_AOCX_PATH\").unwrap_or_else(|_| FPGA_GEN_IMAGE.to_string());\n\n    let aocx = std::fs::read(&amp;aocx_path).unwrap();\n\n    // Create program\n    let mut program =\n        unsafe { Program::create_from_binary(&amp;context, &amp;[device.id()], &amp;[aocx.as_slice()])? };\n\n    // Build the program for the device\n    program.build(&amp;[device.id()], \"\")?;\n    let kernel = Kernel::create(&amp;program, KERNEL_NAME)?;\n\n    // Create OpenCL device buffers\n    let mut input_b = unsafe {\n        Buffer::&lt;cl_float&gt;::create(&amp;context, CL_MEM_READ_ONLY, buffer_size, ptr::null_mut())?\n    };\n    let mut weights_b = unsafe {\n        Buffer::&lt;cl_float&gt;::create(\n            &amp;context,\n            CL_MEM_READ_ONLY,\n            (ksize * ksize) as usize,\n            ptr::null_mut(),\n        )?\n    };\n    let output_b = unsafe {\n        Buffer::&lt;cl_float&gt;::create(&amp;context, CL_MEM_WRITE_ONLY, buffer_size, ptr::null_mut())?\n    };\n\n    let w: cl_uint = width;\n    let h: cl_uint = height;\n\n    // Blocking write\n    let _input_write_event =\n        unsafe { queue.enqueue_write_buffer(&amp;mut input_b, CL_BLOCKING, 0, buffer, &amp;[])? };\n\n    // Non-blocking write, wait for y_write_event\n    let _weights_write_event =\n        unsafe { queue.enqueue_write_buffer(&amp;mut weights_b, CL_NON_BLOCKING, 0, &amp;weights, &amp;[])? };\n\n    let kernel_event = unsafe {\n        kernel.set_arg(0, &amp;input_b)?;\n        kernel.set_arg(1, &amp;output_b)?;\n        kernel.set_arg(2, &amp;weights_b)?;\n        kernel.set_arg(3, &amp;w)?;\n        kernel.set_arg(4, &amp;h)?;\n        kernel.set_arg(5, &amp;ksize)?;\n        queue.enqueue_task(kernel.get(), &amp;[_weights_write_event.get()])?\n    };\n\n    let events: Vec&lt;cl_event&gt; = vec![kernel_event.get()];\n\n    let read_event =\n        unsafe { queue.enqueue_read_buffer(&amp;output_b, CL_NON_BLOCKING, 0, buffer, &amp;events)? };\n\n    // Wait for the read_event to complete.\n    read_event.wait()?;\n\n    // Calculate the kernel duration, from the kernel_event\n    let start_time = kernel_event.profiling_command_start()?;\n    let end_time = kernel_event.profiling_command_end()?;\n    let duration = end_time - start_time;\n    println!(\"kernel execution duration (ns): {}\", duration);\n\n    Ok(())\n}\n\nfn main() -&gt; Result&lt;()&gt; {\n    let mut args = std::env::args().skip(1).collect::&lt;Vec&lt;_&gt;&gt;();\n\n    if args.is_empty() {\n        args.push(String::from(\"--help\")); // help the user out :)\n    }\n    let mut opts = Options::new(args.iter().map(String::as_str));\n    let mut input_path = None;\n    let mut output_path = None;\n\n    while let Some(arg) = opts.next_arg().expect(\"argument parsing error\") {\n        match arg {\n            Arg::Short('h') | Arg::Long(\"help\") =&gt; {\n                eprintln!(\n                    r\"Usage: rust-opencl-fpga [OPTIONS/ARGS] input ...\n                     This command execute an OpenCL Convolution kernel on GPU.\n                     -h, --help   display this help and exit\n                     -o, --output path to record output image\"\n                );\n            }\n            Arg::Short('o') | Arg::Long(\"output\") =&gt; {\n                output_path = opts.value_opt();\n            }\n            Arg::Positional(arg) =&gt; {\n                let metadata = std::fs::metadata(arg);\n                match metadata {\n                    Ok(m) =&gt; {\n                        if !m.is_file() {\n                            panic!(\"{arg:?} is not a file\");\n                        } else {\n                            input_path = Some(arg);\n                        }\n                    }\n                    Err(e) =&gt; {\n                        panic!(\"Error: {e:?}\");\n                    }\n                }\n            }\n            _ =&gt; {}\n        }\n    }\n\n    let (mut buffer, w, h) =\n        load_gray_f32(input_path.unwrap()).expect(\"Cannot read image located at {arg}\");\n    let status = run(&amp;mut buffer, w, h);\n    match status {\n        Ok(_) =&gt; {\n            if let Some(p) = output_path {\n                save_gray_f32(p, &amp;buffer, w, h).expect(\"Cannot save image at {p}\");\n            } else {\n                save_gray_f32(input_path.unwrap(), &amp;buffer, w, h)\n                    .expect(\"Cannot save image at {arg}\");\n            }\n            println!(\"Execution complete\");\n        }\n        Err(e) =&gt; {\n            panic!(\"ClError: {e:?}\");\n        }\n    }\n    Ok(())\n}\n</code></pre>"},{"location":"rust_opencl/#device-code","title":"Device code","text":"./code/rust-opencl-fpga/kernels/conv2d_gray_f32.cl<pre><code>#define MAX_K 31\n\nkernel void conv2d_gray_f32( __global const float* input, \n                             __global float* output, \n                             __global const float* weights, \n                             const int width,  \n                             const int height, \n                             const int kSize) {\n\n    __local float kLocal[MAX_K * MAX_K];\n    #pragma unroll\n    for(int i=0; i&lt;= kSize*kSize; i++){\n\n            kLocal[i] = weights[i];\n\n    }\n\n    float acc;\n    const int r = (kSize-1) / 2;\n    int ii,jj;\n    for( int i = r; i &lt; height-r; i++){\n       for(int j = r; j &lt; width-r; j++){\n            acc = 0;\n            #pragma unroll\n            for( int k = 0; k &lt; kSize*kSize; k++){\n                jj = (k%kSize) + j - r;\n                ii = (k/kSize) + i - r;\n                acc += input[ii*width+jj] * kLocal[k];\n            }\n            output[i*width+j] = acc;\n        }\n    }\n  }\n</code></pre> ./code/rust-opencl-fpga/build.rs./code/rust-opencl-fpga/build.rs<pre><code>use std::{\n    env, fs,\n    path::{Path, PathBuf},\n    process::Command,\n};\n\nfn main() {\n    println!(\"cargo:rerun-if-changed=kernels/my_kernel.cl\");\n    println!(\"cargo:rerun-if-env-changed=AOC\");\n    println!(\"cargo:rerun-if-env-changed=AOC_FLAGS\");\n\n    let kernel_name = \"conv2d_gray_f32\";\n\n    let aoc = env::var(\"AOC\").unwrap_or_else(|_| \"aoc\".to_string());\n    let aoc_flags = env::var(\"AOC_FLAGS\").unwrap_or_default();\n\n    let kernel_filename = format!(\"kernels/{}.cl\", kernel_name);\n    let kernel_src = Path::new(kernel_filename.as_str());\n\n    let out_dir = PathBuf::from(env::var(\"OUT_DIR\").expect(\"OUT_DIR not set\"));\n    let built = out_dir.join(format!(\"{}.aocx\", kernel_name)); // adjust extension as needed\n\n    // 1) Build into OUT_DIR\n    let mut cmd = Command::new(aoc);\n\n    if !aoc_flags.trim().is_empty() {\n        for part in aoc_flags.split_whitespace() {\n            cmd.arg(part);\n        }\n    }\n\n    cmd.arg(kernel_src).arg(\"-o\").arg(&amp;built);\n\n    eprintln!(\"Running: {:?}\", cmd);\n\n    let status = cmd.status().expect(\"failed to run aoc\");\n    if !status.success() {\n        panic!(\"aoc failed: {status}\");\n    }\n\n    // 2) Copy into a stable location under target/\n    // CARGO_TARGET_DIR may be set; if not, default is \"target\".\n    let target_dir = env::var(\"CARGO_TARGET_DIR\").unwrap_or_else(|_| \"target\".to_string());\n    let profile = env::var(\"PROFILE\").expect(\"PROFILE not set\"); // \"debug\" or \"release\"\n\n    let stable_dir = Path::new(&amp;target_dir).join(\"aoc\").join(&amp;profile);\n    fs::create_dir_all(&amp;stable_dir).expect(\"failed to create stable aoc dir\");\n\n    let stable_path = stable_dir.join(format!(\"{}.aocx\", kernel_name));\n    fs::copy(&amp;built, &amp;stable_path).expect(\"failed to copy aoc output\");\n\n    // 3) Tell Rust code where the separate file is\n    println!(\"cargo:rustc-env=FPGA_AOCX_PATH={}\", stable_path.display());\n}\n</code></pre>"},{"location":"rust_opencl/#execution-on-meluxina","title":"Execution on MeluXina","text":"<p>What about memory accesses in FPGA ? </p> <ul> <li>In order to use Direct Memory Access (DMA), we need to setup proper data alignment or the offline compiler will output the following warnings: <pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\n</code></pre> To do so, we use jemalloc: <pre><code>module load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./exe\n</code></pre></li> </ul>"},{"location":"rust_opencl/#interactive-execution","title":"Interactive execution","text":"<pre><code>salloc -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; -t 30:00 -q default -p fpga\ncd RustOnAccelerators/code\nsource setup_rustfpga.sh\n</code></pre> Building for EmulationExecution on FPGA card <pre><code>cd ${CODE_ROOT}/rust-opencl-fpga \nAOC=\"$(which aoc)\" AOC_FLAGS=\"-v -march=emulator -legacy-emulator -board=p520_hpc_m210h_g3x16\" cargo build --release\n# Execute the code\nCL_CONTEXT_EMULATOR_DEVICE_INTELFPGA=1 ./target/release/rust-opencl-fpga -orust-opencl-fpga.png ../../data/original_image.png\n</code></pre> <pre><code>cd ${CODE_ROOT}/rust-opencl-fpga \n# Execute the code\nLD_PRELOAD=${JEMALLOC_PRELOAD} FPGA_AOCX_PATH=${HARD_IMAGE} ./target/release/rust-opencl-fpga -orust-opencl-fpga.png ../../data/original_image.png\n</code></pre>"},{"location":"rust_opencl/#batch-execution","title":"Batch execution","text":"<pre><code>cd RustOnAccelerators/code\nsbatch -A &lt;project_name&gt; --reservation=&lt;reservation_name&gt; launcher-rust-opencl-fpga.sh\n</code></pre>"},{"location":"rust_opencl/#results","title":"Results","text":"<ul> <li>You should see the following results for both executions:</li> </ul> Original Convolution"},{"location":"rust_opencl/#explore-further","title":"Explore Further","text":"<ul> <li>Modify the host and the device code to run on GPU using OpenCL</li> </ul> <p>Info</p> <p>Look inside the <code>code</code> folder for the solutions. We have already prepared the solution for you.</p>"}]}